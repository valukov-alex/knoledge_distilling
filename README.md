# Knoledge distillation
Задание для VK Lab  
Эксперименты с [knowlege distilation](https://arxiv.org/abs/1503.02531)
## Как запускать
Можно склонировать репрозиторий и запустить knoledge_distilling.ipynb  
Либо тыкнуть сюда [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/valukov-alex/knoledge_distilling/blob/master/knoledge_distilling.ipynb)
## Краткий обзор
В статье авторы делают предположение о том, что выученнная большая сеть (или алгоритм/ансамбль)
на каждом объекте может выдавать информацию о том, как этот объект похож на другие классы
(кроме своего). Эта информация содержится в распределении вероятностей, получаемых на выходе.  
Таким образом, помимо "твердых" лейблов с 1 в правильном классе и 0 в остальных,
предлагается использовать "мягкие" - распределение вероятностей от обучающего алгоритма.
Использовав сочетание мягких лейблов с твердыми можно обучить небольшой алгоритм/сеть быстрее
и добиться лучших результатов.
![formula](https://render.githubusercontent.com/render/math?math=%5Clarge%20loss%20=%20alpha%20*%20soft%5C_loss%20*%20T%5E2%20+%20(1%20-%20alpha)%20*%20hard%5C_loss)
![formula](https://render.githubusercontent.com/render/math?math=\large%20soft%5C_loss%20=%20CrossEntropy(Softmax(distill%5C_logits/T),%20Softmax(teacher%5C_logits/T)))
## Задача
В качестве задачи я выбрал классификацию изображений на датасете [imagewoof](https://github.com/fastai/imagenette).  
Выбрал изображения, т.к. раньше работал с ними.
## Эксперименты
Цель эксперимента была не столько добиться найлучшего результата на тестовой выборке,
сколько проверить гипотезу о том, что с KD сеть учится быстрее.  
В качестве обучающей сети была выбранна resnet50 с предобученными весами. Я
заменил последний линейный слой и доучил.  
В качестве дистилированной модели была выбрана простая cnn с 4-мя свертками.  
Проделал серию экспериментов, чтобы посмотреть на скорость обучения при различных alpha, T.
Учил сети с нуля, 30 эпох на каждый эксперимент.
## Результаты
Результаты для разных alpha с фиксированным T, и для разных T с фиксированным alpha:
![plot T=20](https://raw.githubusercontent.com/valukov-alex/knoledge_distilling/master/plots/plot_T20.png)
![plot alpha=0.5](https://raw.githubusercontent.com/valukov-alex/knoledge_distilling/master/plots/plot_alpha0.5.png)
Таблица с итоговыми результатами:
| Params | accuracy, % |
| :---: | :---: |
| hard | 40.2 |
| a=0.1 T=20 | 40.7 |
| a=0.5 T=20 | 41.3 |
| a=0.9 T=20 | 40.7 |
| a=0.99 T=20 | 41.8 |
| a=0.5 T=1 | 41.4 |
| a=0.5 T=5 | 42.3 |
| a=0.5 T=100 | 39.8 |
## Выводы
В целом можно заметить, что с KD сеть учится быстрее. Более точных результатов можно
добиться, лучше обучив большую сеть (например поиграться с learning rate, уменьшая на каждой эпохе).
Аналогично можно посмотреть на результаты, когда дистилированная сеть сойдется.

 
